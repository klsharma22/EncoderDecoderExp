{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Machine Translation using Google's Encoder Decoder Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing all the dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import pandas as pd\n",
    "import tarfile\n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Turning on mixed precision policy for better performance"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "tf.keras.mixed_precision.set_global_policy('mixed_float16')"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting one with the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Extracting all the files from the tar file\n",
    "if not os.path.exists('/Users/klsharma22/Desktop/EncoderDecoderExp/wiki'):\n",
    "    tar_ref = tarfile.TarFile('/Users/klsharma22/Desktop/EncoderDecoderExp/wiki-titles.tar')\n",
    "    tar_ref.extractall()\n",
    "    print('Files extracted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Extracting data from the file\n",
    "with open('/Users/klsharma22/Desktop/EncoderDecoderExp/wiki/hi-en/wiki-titles.hi-en', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Separating english from hindi\n",
    "hin_sentences = [line.split('|||')[0] for line in lines]\n",
    "eng_sentences = [line.split('|||')[1][:-1] for line in lines]\n",
    "len(hin_sentences), len(eng_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising the dataset we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "random_idx = random.choices(range(len(hin_sentences)), k= 5)\n",
    "for idx in random_idx:\n",
    "    print(f\"English sentence: {eng_sentences[idx]}\")\n",
    "    print(f\"Hindi sentce: {hin_sentences[idx]}\")\n",
    "    print('--------------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Lets look at the distibution of number of words per sentence\n",
    "eng_words_sentences = [len(sentence.split()) for sentence in eng_sentences]\n",
    "hin_word_sentences = [len(sentence.split()) for sentence in hin_sentences]\n",
    "\n",
    "len(eng_words_sentences), len(hin_word_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Create vocabulary set for both the language\n",
    "eng_vocab = set()\n",
    "\n",
    "for line in eng_sentences:\n",
    "    for word in line.split():\n",
    "        eng_vocab.add(word)\n",
    "\n",
    "eng_vocab.add('<SOS>')\n",
    "eng_vocab.add('<EOS>')\n",
    "len(eng_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "english_dictionary = pd.DataFrame(eng_vocab)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Cross verifying the values\n",
    "english_dictionary.nunique()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Storing vocab value of hindi same as english\n",
    "hin_vocab = set()\n",
    "\n",
    "for line in hin_sentences:\n",
    "    for word in line.split():\n",
    "        hin_vocab.add(word)\n",
    "\n",
    "len(hin_vocab)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Cross verifying\n",
    "hindi_dictionary = pd.DataFrame(hin_vocab)\n",
    "hindi_dictionary.nunique()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Plotting number of words distribution\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(eng_words_sentences)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(hin_word_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Let's cover the maximum number of data\n",
    "max_len_eng = max(eng_words_sentences)\n",
    "max_len_hin = max(hin_word_sentences)\n",
    "max_len_eng, max_len_hin"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocess the data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentences: list):\n",
    "    return ['<SOS> ' + sentence.strip() + ' <EOS>' for sentence in sentences]"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "eng_sentences_preprocessed = preprocess_sentence(eng_sentences)\n",
    "hin_sentences_preprocessed = preprocess_sentence(hin_sentences)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "random_idx = random.randint(0, len(eng_sentences_preprocessed) - 1)\n",
    "print(eng_sentences_preprocessed[random_idx])\n",
    "print(hin_sentences_preprocessed[random_idx])"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Encoder class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, vocab, embedding_size, units, encoding_layers, **kwargs):\n",
    "        # initialisation of the variables\n",
    "        super().__init__(**kwargs)\n",
    "        self.vocab = vocab\n",
    "        self.units = units\n",
    "        self.encoding_layers = encoding_layers\n",
    "        self.embedding_size = embedding_size\n",
    "        # self.output = None\n",
    "\n",
    "        #initialisation of the layers required\n",
    "        # self.input_layer = layers.Input(shape= (None, ), dtype= tf.int32)\n",
    "        self.embedding = layers.Embedding(input_dim= self.vocab,\n",
    "                                          output_dim= self.embedding_size,name= 'encoder_embedding_layer')\n",
    "        self.lstm_init_units = 2 * (self.embedding_size + self.units)\n",
    "        self.lstm_layers_recurrent = []\n",
    "        self.lstm_layers_recurrent.append(layers.LSTM(self.lstm_init_units, return_sequences=True, name= 'lst_layers_recurrent_0'))\n",
    "        self.lstm_units = self.lstm_init_units\n",
    "        for _ in range(self.encoding_layers - 3):\n",
    "            self.lstm_units += (self.embedding_size + self.units)\n",
    "            self.lstm_layers_recurrent.append(layers.LSTM(self.lstm_units, return_sequences=True, name=f'lstm_layers_recurrent_{_ + 1}'))\n",
    "        self.lstm_layer_non_recurrent = layers.LSTM(self.units, return_sequences=False, return_state= True, name= 'lstm_layer_non_recurrent')\n",
    "        self.bilst_layer = layers.Bidirectional(layers.LSTM(self.units // 2, return_sequences=True), name= 'bilst_layer')\n",
    "        self.concatenate_layer = layers.Concatenate(name= 'concatenate_layer')\n",
    "        self.dropout_layer = layers.Dropout(0.5, name= 'dropout_layer')\n",
    "        \n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.embedding(inputs)\n",
    "        bilstm_output = self.bilst_layer(x)\n",
    "        x = self.dropout_layer(bilstm_output)\n",
    "        x = self.concatenate_layer([bilstm_output, x])\n",
    "        for i in range(self.encoding_layers - 2):\n",
    "            lstm_layer_output = self.lstm_layers_recurrent[i](x)\n",
    "            lstm_layer_output = self.dropout_layer(lstm_layer_output)\n",
    "            x = self.concatenate_layer([lstm_layer_output, x])\n",
    "            \n",
    "        x, h, c = self.lstm_layer_non_recurrent(x)\n",
    "        output = self.dropout_layer(x)\n",
    "        # x = tf.keras.layers.Dense(15, activation= 'softmax')(x)\n",
    "        \n",
    "        return output, h, c\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(eng_sentences_preprocessed, hin_sentences_preprocessed, test_size= 0.2,\n",
    "                                                    random_state= 42)\n",
    "len(X_train), len(X_test), len(y_train), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Creating a text vectorization for the source language\n",
    "eng_vectorizer = layers.TextVectorization(max_tokens= len(eng_vocab),\n",
    "                                          pad_to_max_tokens= True,\n",
    "                                          output_sequence_length= max_len_eng + 2,\n",
    "                                          name= 'eng_vectorizer')\n",
    "\n",
    "eng_vectorizer.adapt(X_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# A sample action of our text vectorization\n",
    "random_text = random.choice(X_train)\n",
    "print(f\"Original text: {random_text}\")\n",
    "print(f\"Vectorized text: {eng_vectorizer(random_text)}\")\n",
    "print(f\"Vector shape: {eng_vectorizer(random_text).shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Creating an instance of our encoder layer\n",
    "encoder_layer = Encoder(vocab= len(eng_vocab),\n",
    "                        embedding_size= 128,\n",
    "                        units= 512,\n",
    "                        encoding_layers= 8,\n",
    "                        name= 'encoder_layer', trainable= True)\n",
    "\n",
    "\n",
    "encoder_layer.get_config()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(len(X_train[0].split()))\n",
    "print(eng_vectorizer(X_train[:128]).shape)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "with tf.device('GPU:0'):\n",
    "    output, final_memory_state, final_carry_state = encoder_layer(eng_vectorizer(X_train[:128]))\n",
    "    \n",
    "print(output.shape, final_memory_state.shape, final_carry_state.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "hin_vectorizer = tf.keras.layers.TextVectorization(max_tokens= len(hin_vocab),\n",
    "                                                   pad_to_max_tokens= True,\n",
    "                                                   output_sequence_length= max_len_hin,\n",
    "                                                   name= 'hin_vectorizer')\n",
    "\n",
    "hin_vectorizer.adapt(y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "random_text = random.choice(y_train)\n",
    "\n",
    "print(f\"Hindi text: {random_text}\")\n",
    "print(f\"Vectorize text: {hin_vectorizer(random_text)}\")\n",
    "print(f\"Vector shape: {hin_vectorizer(random_text).shape}\")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, vocab_size, embedding_size, decoding_layers, initial_state_size, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        # self.initial_state = initial_state\n",
    "        self.units = units\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.decoding_layers = decoding_layers\n",
    "        self.initial_state_size = initial_state_size\n",
    "        \n",
    "        # initializing all the layers\n",
    "        self.initial_state = tf.keras.layers.Input(shape= (self.initial_state_size,), name= 'decoder_initial_state')\n",
    "        self.embedding = tf.keras.layers.Embedding(input_dim= self.vocab_size,\n",
    "                                                   output_dim= self.embedding_size,\n",
    "                                                   name= 'decoder_embedding_layer')\n",
    "        # self.decoder_input = tf.keras.layers.Input(shape= (None, ), name= 'decoder_input', dtype= 'string')\n",
    "        self.concatenate_layer = tf.keras.layers.Concatenate(name= 'concatenate_layer')\n",
    "        self.lstm_layers_recurrent = []\n",
    "        for i in range(self.decoding_layers):\n",
    "            self.lstm_layers_recurrent.append(tf.keras.layers.LSTM(self.units, return_sequences=True, name= f'decoder_lstm_layer_{i}'))\n",
    "            \n",
    "        self.dense = tf.keras.layers.Dense(vocab_size, activation= 'softmax', name='decode_output_layer')\n",
    "        \n",
    "        \n",
    "    def call(self, inputs, initial_state= None):\n",
    "        if initial_state == None:\n",
    "            initial_state = self.initial_state\n",
    "        x = self.embedding(inputs)\n",
    "        x = self.lstm_layers_recurrent[0](x, initial_state=initial_state)\n",
    "        x = self.lstm_layers_recurrent[1](x, initial_state=initial_state)\n",
    "        for i in range(2, self.decoding_layers - 1):\n",
    "            lstm_output = self.lstm_layers_recurrent[i](x, initial_state)\n",
    "            x = self.concatenate_layer([x, lstm_output])\n",
    "        \n",
    "        x = self.lstm_layers_recurrent[-1](x)\n",
    "        x = self.dense(x)\n",
    "        \n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "decoder_layer = Decoder(units= 512,\n",
    "                        vocab_size= len(hin_vocab),\n",
    "                        embedding_size= 128,\n",
    "                        decoding_layers= 8,\n",
    "                        initial_state_size= 512,\n",
    "                        name= 'decoding_layer',\n",
    "                        trainable= True)\n",
    "decoder_layer.get_config()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    encoder_output, final_memory_state, final_carry_state = encoder_layer(eng_vectorizer(X_train[:128]))\n",
    "    decoder_output = decoder_layer(hin_vectorizer(y_train[:128]), initial_state= [final_memory_state, final_carry_state])\n",
    "    \n",
    "decoder_output.shape"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Creation\n",
    "\n",
    "We have created encoder and decoder as layer in the above code using subclass method and also we have created text vecotrizer usign keras `TextVectorization` layer.\n",
    "\n",
    "Let's list the component in the order we want to build the mode:\n",
    "1. Encoder Text Vectorization\n",
    "2. Encoder layer with in built embedding layer\n",
    "3. Decoder Text Vectorizartion\n",
    "4. Decoder layer connected with in built embedding layer and feature for initial state for encoder output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Creating the entire encoder decoder model\n",
    "inputs_encoder = tf.keras.layers.Input(shape=(1,  ), dtype= 'string', name= 'encoder_input_layer')\n",
    "print(inputs_encoder.shape)\n",
    "encoder_text_vectors = eng_vectorizer(inputs_encoder)\n",
    "print(encoder_text_vectors.shape)\n",
    "encoder_outputs, final_memory_states, final_carry_states = encoder_layer(encoder_text_vectors)\n",
    "print(encoder_outputs.shape, final_memory_states.shape, final_carry_states)\n",
    "\n",
    "inputs_decoder =  tf.keras.layers.Input(shape= (1,  ), dtype= 'string', name= 'decoder_input_layer')\n",
    "print(inputs_decoder.shape)\n",
    "decoder_text_vectors = hin_vectorizer(inputs_decoder)\n",
    "print(decoder_text_vectors.shape)\n",
    "decoder_outputs = decoder_layer(decoder_text_vectors, initial_state= [final_memory_states, final_carry_states])\n",
    "print(decoder_outputs.shape)\n",
    "\n",
    "model = tf.keras.Model(inputs = [inputs_encoder, inputs_decoder], outputs =[decoder_outputs])\n",
    "\n",
    "model.compile(loss= 'sparse_categorical_crossentropy',\n",
    "              optimizer= tf.keras.optimizers.RMSprop(learning_rate= 1e-3))\n",
    "\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model, show_layer_names= True)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creating dataset for faster and better training of the model\n",
    "\n",
    "Using `tf.data` API, we are going to create a better pipeline to train the model usign batch and prefetch method"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_labels = hin_vectorizer(y_train)\n",
    "test_labels = hin_vectorizer(y_test)\n",
    "len(X_train[0].split()), len(y_train), train_labels.shape, len(X_test), len(y_test), test_labels.shape"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_data = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "train_labels = tf.data.Dataset.from_tensor_slices(train_labels)\n",
    "train_dataset = tf.data.Dataset.zip(train_data, train_labels).batch(128).prefetch(tf.data.AUTOTUNE)\n",
    "valid_data = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "valid_labels = tf.data.Dataset.from_tensor_slices(test_labels)\n",
    "valid_dataset = tf.data.Dataset.zip(valid_data, valid_labels).batch(128).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "train_dataset, valid_dataset\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    history = model.fit(train_dataset,\n",
    "                      epochs= 5,\n",
    "                      validation_data= valid_dataset,\n",
    "                        validation_steps= int(0.1 * len(valid_dataset)),\n",
    "                      callbacks= [tf.keras.callbacks.ModelCheckpoint('GNMT_exp.weights.h5', monitor='val_loss', save_best_only=True, save_weights_only=True),\n",
    "                                  tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience= 3, verbose= 1),\n",
    "                                  tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', patience= 2, verbose= 1)])"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model.load_weights('GNMT_exp.weights.h5')\n",
    "model.evaluate(valid_dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model_pred = model.predict(valid_dataset)\n",
    "model_pred.shape"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model_preds = model.predict([tf.expand_dims(X_test[-1], axis= 0), tf.expand_dims('<SOS>', axis= 0)])\n",
    "model_preds.shape"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model_preds = tf.argmax(tf.squeeze(model_preds), axis= 1)\n",
    "model_preds"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "vocab = hin_vectorizer.get_vocabulary()\n",
    "\" \".join([vocab[pred.numpy()] for pred in model_preds])"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
