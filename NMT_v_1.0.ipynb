{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Creating a mind map before getting into action\n",
    "\n",
    "So after coming across from the mistakes before this notebook, let's plan what all things we need to do so that we perform the crucial steps needed for the NMT to make with all the proper components and the preprocessed data.\n",
    "\n",
    "**Data Preprocessing Actions**:\n",
    "1. Shift the target texts.\n",
    "2. Pad the text with proper annotations.\n",
    "\n",
    "**Components of the model**:\n",
    "1. Create *text vectors* of both source and target language separately using TensorFlow `TextVectorization` layer.\n",
    "2. Create *embedding layers* of both source and target language separately using TensorFlow `Embedding` layer.\n",
    "3. Create a class known *Encoder* which inherits the properties of TensorFlow class `Layer`. In the sub-class method of creating a layer, we are going to develop the encoder architecture of the NMT. (The architecture will be discussed while we are developing the layer.)\n",
    "4. Create a class known as *Decoder* which inherits the properties of TensorFlow class `Layer`. In this sub-class method of creating a layer, we are going to develop the decoder architecture of the NMT. (The architecture will be discussed while we are developing the layer.)\n",
    "5. After creating all the components, we are going to create a class called *EncoderDecoder* which will inherit from TensorFlow class `Model`. In this class we will assemble all of the layers that we have prepared in the above steps and then create a custom call function which will allow us to train the decoder layer as we expect it to do.\n",
    "6. Create an instance of the *EncoderDecoder* class with all the parameters passed in its constructor and compile the model with `tf.keras.losses.SparseCategoricalCrossentropy()` as loss function and `tf.keras.optimizers.RMSprop()` as the optimizer.\n",
    "7. We will then create the *train_dataset* and *valid_dataset* using TensorFlow `tf.data` API for better performance of the model training.\n",
    "8. We will fit the model with the training data with 5 epochs and *callbacks* such as `tf.keras.callbacks.ModelCheckPoint`, `tf.keras.callbacks.EarlyStopping`, `tf.keras.callbacks.ReduceLROnPlateau` and the validation dataset.\n",
    "9. After training the model we will evaluate the model using different metrics system and predict with an unseen data and observe the quality of the prediction."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1e240db960ed8729"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Importing all the necessary libraries"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c9430d4d813aceb5"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import tarfile\n",
    "import random"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T20:34:07.172887Z",
     "start_time": "2024-04-04T20:34:07.170737Z"
    }
   },
   "id": "bfdf0bb37b8c825a",
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Start the project\n",
    "\n",
    "As all machine learning project has two phases, this is project is no exception for it. We will first work on the *Data Preprocessing* and then *Model Development*"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "64a05be9d25b00fc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Preprocessing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2f9de603448c9b31"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Creating a constant which contains the starting path of the project so that\n",
    "START_PATH = str(os.getcwd()) + '/'\n",
    "COMP_DATA_PATH = os.path.join(START_PATH, 'wiki-titles.tgz')\n",
    "DATA_URL = 'https://www.statmt.org/wmt14/wiki-titles.tgz'\n",
    "DATA_DIR = os.path.join(START_PATH, 'wiki/hi-en/wiki-titles.hi-en')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T20:23:21.601247Z",
     "start_time": "2024-04-04T20:23:21.598970Z"
    }
   },
   "id": "e37d49d18a6631f1",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Loading Data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9eaef38bbde5b36b"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.statmt.org/wmt14/wiki-titles.tgz\n",
      "--2024-04-05 01:53:21--  https://www.statmt.org/wmt14/wiki-titles.tgz\r\n",
      "Resolving www.statmt.org (www.statmt.org)... 129.215.32.28\r\n",
      "Connecting to www.statmt.org (www.statmt.org)|129.215.32.28|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 8168057 (7.8M) [application/x-gzip]\r\n",
      "Saving to: ‘wiki-titles.tgz’\r\n",
      "\r\n",
      "wiki-titles.tgz     100%[===================>]   7.79M   308KB/s    in 14s     \r\n",
      "\r\n",
      "2024-04-05 01:53:38 (555 KB/s) - ‘wiki-titles.tgz’ saved [8168057/8168057]\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(COMP_DATA_PATH):\n",
    "    print(f'Downloading data from {DATA_URL}')\n",
    "    !wget https://www.statmt.org/wmt14/wiki-titles.tgz\n",
    "else:\n",
    "    print(\"Data already downloaded\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T20:23:38.248524Z",
     "start_time": "2024-04-04T20:23:21.601962Z"
    }
   },
   "id": "43050224ec71df74",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file exists\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(COMP_DATA_PATH):\n",
    "    print(\"File does not exist\")\n",
    "else:\n",
    "    print(\"The file exists\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T20:23:38.254280Z",
     "start_time": "2024-04-04T20:23:38.250060Z"
    }
   },
   "id": "5f1139795316c182",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File extracted\n"
     ]
    }
   ],
   "source": [
    "with tarfile.open(COMP_DATA_PATH, 'r') as tar_ref:\n",
    "    tar_ref.extractall()\n",
    "    print(\"File extracted\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T20:23:38.378855Z",
     "start_time": "2024-04-04T20:23:38.256821Z"
    }
   },
   "id": "a0c519a809d4a9ae",
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Extracting data from file "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c5f8236ca4c75de5"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "['० जनवरी ||| January 0\\n',\n '० मार्च ||| March 0\\n',\n '१००० ||| 1000\\n',\n '१००१ ||| 1001\\n',\n '१००२ ||| 1002\\n']"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extracting the lines from the file of the dataset\n",
    "with open(DATA_DIR, 'r') as f:\n",
    "    lines = f.readlines()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T20:29:22.581913Z",
     "start_time": "2024-04-04T20:29:22.568275Z"
    }
   },
   "id": "8706972bebef0f20",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "(['January 0',\n  'March 0',\n  '1000',\n  '1001',\n  '1002',\n  '1003',\n  '1004',\n  '1005',\n  '1006',\n  '1007'],\n ['० जनवरी',\n  '० मार्च',\n  '१०००',\n  '१००१',\n  '१००२',\n  '१००३',\n  '१००४',\n  '१००५',\n  '१००६',\n  '१००७'],\n 32863,\n 32863)"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Splitting the data into two list of source language and target language\n",
    "src_senteces = [line.split('|||')[1][1:-1] for line in lines]\n",
    "trg_sentences = [line.split('|||')[0][:-1] for line in lines]\n",
    "src_senteces[:10], trg_sentences[:10], len(src_senteces), len(trg_sentences)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T20:34:33.385016Z",
     "start_time": "2024-04-04T20:34:33.360278Z"
    }
   },
   "id": "1878ae100a6b426f",
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Visualise the data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d9c90fb2692dc8df"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source sentence: 217 BC\n",
      "Target sentence: २१७ ईसा पूर्व\n"
     ]
    }
   ],
   "source": [
    "def visualise_random_sentences(src_sent, trg_sent):    \n",
    "    random_idx = random.randint(0, len(src_sent))\n",
    "    \n",
    "    print(f\"Source sentence: {src_sent[random_idx]}\")\n",
    "    print(f\"Target sentence: {trg_sent[random_idx]}\")\n",
    "    \n",
    "visualise_random_sentences(src_sent= src_senteces,\n",
    "                    trg_sent= trg_sentences)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T20:40:00.027243Z",
     "start_time": "2024-04-04T20:40:00.022761Z"
    }
   },
   "id": "36e32e67bba57146",
   "execution_count": 36
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Shift the target data with one token"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fd558fc2e24c14d4"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source sentence: Arsenic\n",
      "Target sentence: <SOS> आर्सेनिक <EOS>\n"
     ]
    }
   ],
   "source": [
    "trg_sentences_preprocessed = ['<SOS> ' + sentence + ' <EOS>' for sentence in trg_sentences]\n",
    "visualise_random_sentences(src_sent= src_senteces,\n",
    "                           trg_sent= trg_sentences_preprocessed)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T20:41:52.914892Z",
     "start_time": "2024-04-04T20:41:52.906244Z"
    }
   },
   "id": "9c752ba428dc9646",
   "execution_count": 43
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max source sentence length: 13\n",
      "Max target sentence length: 17\n"
     ]
    }
   ],
   "source": [
    "# Calculate the max length of the text for each language lists\n",
    "src_word_per_sentence = [len(line.split()) for line in src_senteces]\n",
    "trg_word_per_sentence = [len(line.split()) for line in trg_sentences_preprocessed]\n",
    "\n",
    "max_src_len = max(src_word_per_sentence)\n",
    "max_trg_len = max(trg_word_per_sentence)\n",
    "\n",
    "print(f\"Max source sentence length: {max_src_len}\")\n",
    "print(f\"Max target sentence length: {max_trg_len}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T20:47:52.059166Z",
     "start_time": "2024-04-04T20:47:52.038322Z"
    }
   },
   "id": "86d7ec2f1690e76",
   "execution_count": 45
  },
  {
   "cell_type": "markdown",
   "source": [
    ">**Note**: Padding of the data will be done after vectorising the text data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2e5640df1a236aa"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model development\n",
    "\n",
    "We have completed the data preprocessing of the data and are ready to develop the model. Let's discuss the steps we are going to take for it:\n",
    "1. Create individual components\n",
    "2. Assemble the model\n",
    "3. Create Data Pipeline\n",
    "4. Fit the model\n",
    "5. Evaluate the model\n",
    "6. Predict using the model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "96e38c8a72bd5d08"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
